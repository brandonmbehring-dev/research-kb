name: Weekly Full Rebuild & Validation

on:
  schedule:
    - cron: '0 2 * * 0'  # 2 AM every Sunday
  workflow_dispatch:  # Manual trigger

jobs:
  full-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    services:
      postgres:
        image: ankane/pgvector:latest
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: research_kb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Free disk space
        run: |
          echo "=== Disk space before cleanup ==="
          df -h /
          # Remove large pre-installed packages we don't need
          sudo rm -rf /usr/share/dotnet || true
          sudo rm -rf /usr/local/lib/android || true
          sudo rm -rf /opt/ghc || true
          sudo rm -rf /opt/hostedtoolcache/CodeQL || true
          sudo rm -rf /usr/local/share/boost || true
          sudo apt-get clean || true
          echo "=== Disk space after cleanup ==="
          df -h /

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e packages/cli
          pip install -e packages/storage
          pip install -e packages/pdf-tools
          pip install -e packages/extraction
          pip install -e packages/contracts
          pip install -e packages/common
          pip install pytest pytest-asyncio pyyaml

      - name: Drop and recreate database (clean slate)
        env:
          PGPASSWORD: postgres
        run: |
          psql -h localhost -U postgres -c "DROP DATABASE IF EXISTS research_kb"
          psql -h localhost -U postgres -c "CREATE DATABASE research_kb"

      - name: Run database migrations
        env:
          PGPASSWORD: postgres
        run: |
          # Apply schema
          if [ -f packages/storage/schema.sql ]; then
            psql -h localhost -U postgres -d research_kb < packages/storage/schema.sql
          fi
          # Apply migrations
          for migration in packages/storage/migrations/*.sql; do
            if [ -f "$migration" ]; then
              echo "Applying migration: $migration"
              psql -h localhost -U postgres -d research_kb < "$migration"
            fi
          done

      - name: Start embedding server
        run: |
          # Start server in background (model download ~1.3GB may take time)
          echo "Starting embedding server (model will download on first run)..."
          python -m research_kb_pdf.embed_server &
          SERVER_PID=$!

          # Wait for server to be ready (JSON protocol over Unix socket)
          echo "Waiting for embedding server to be ready..."
          for i in {1..60}; do
            if [ -S /tmp/research_kb_embed.sock ]; then
              # Send ping request via Python (server uses JSON, not HTTP)
              RESPONSE=$(python3 -c "
          import socket, json
          try:
              s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
              s.connect('/tmp/research_kb_embed.sock')
              s.sendall(json.dumps({'action': 'ping'}).encode())
              s.shutdown(socket.SHUT_WR)
              data = s.recv(4096)
              s.close()
              resp = json.loads(data)
              print(resp.get('status', 'error'))
          except Exception as e:
              print(f'error: {e}')
          " 2>/dev/null)
              if [ "$RESPONSE" = "ok" ]; then
                echo "âœ… Embedding server ready (attempt $i)"
                exit 0
              fi
            fi
            echo "Waiting... (attempt $i/60)"
            sleep 5
          done

          echo "ERROR: Embedding server failed to start after 5 minutes"
          exit 1

      - name: Set up Ollama (optional, for concept extraction)
        continue-on-error: true
        run: |
          # Download and install Ollama
          curl -fsSL https://ollama.com/install.sh | sh || echo "Ollama installation failed"
          # Start Ollama server
          ollama serve > /dev/null 2>&1 &
          sleep 5
          # Pull model (this might take a while)
          ollama pull llama3.1:8b || echo "Failed to pull Ollama model"

      - name: "CRITICAL: Ingest corpus (~500 chunks)"
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_DB: research_kb
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        timeout-minutes: 15
        run: |
          if [ -f scripts/ingest_corpus.py ]; then
            python scripts/ingest_corpus.py || \
              (echo "ERROR: Corpus ingestion failed" && exit 1)
          else
            echo "WARNING: ingest_corpus.py not found"
          fi

      - name: "Validate retrieval quality"
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_DB: research_kb
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        timeout-minutes: 5
        run: |
          if [ -f scripts/eval_retrieval.py ]; then
            python scripts/eval_retrieval.py || \
              (echo "ERROR: Retrieval validation failed" && exit 1)
          fi

      - name: "Extract concepts (limit 1000 chunks)"
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_DB: research_kb
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        timeout-minutes: 20
        run: |
          if [ -f scripts/extract_concepts.py ]; then
            # Run with limit to avoid timeout
            timeout 18m python scripts/extract_concepts.py --limit 1000 || \
              (echo "ERROR: Concept extraction timed out or failed" && exit 1)
          fi

      - name: "Validate concept extraction quality"
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_DB: research_kb
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        timeout-minutes: 5
        run: |
          if [ -f scripts/validate_seed_concepts.py ]; then
            python scripts/validate_seed_concepts.py --output json || \
              (echo "ERROR: Seed concept validation failed" && exit 1)
          fi

      - name: "Validate knowledge graph"
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_DB: research_kb
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        timeout-minutes: 3
        run: |
          if [ -f scripts/master_plan_validation.py ]; then
            python scripts/master_plan_validation.py || \
              (echo "ERROR: Knowledge graph validation failed" && exit 1)
          fi

      - name: Run all script tests
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_DB: research_kb
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        timeout-minutes: 5
        run: |
          pytest tests/scripts/ -v --tb=short || \
            echo "Script tests completed with warnings"

      - name: Run all CLI tests
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_DB: research_kb
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        timeout-minutes: 5
        run: |
          pytest packages/cli/tests/ -v --tb=short || \
            echo "CLI tests completed with warnings"

      - name: Cache database dump for daily validation
        env:
          PGPASSWORD: postgres
        run: |
          pg_dump -h localhost -U postgres research_kb > db_dump.sql

      - name: Upload database cache
        uses: actions/cache/save@v4
        with:
          path: db_dump.sql
          key: weekly-db-latest

      - name: Generate audit report
        if: always()
        run: |
          echo "# Weekly Validation Report" > audit_report.md
          echo "Date: $(date)" >> audit_report.md
          echo "" >> audit_report.md
          echo "## Build Summary" >> audit_report.md
          echo "- Run number: ${{ github.run_number }}" >> audit_report.md
          echo "- Status: ${{ job.status }}" >> audit_report.md

      - name: Upload audit report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: weekly-audit-report
          path: audit_report.md
          retention-days: 90
